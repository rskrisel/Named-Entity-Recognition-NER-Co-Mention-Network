{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rskrisel/Named-Entity-Recognition-NER-Co-Mention-Network/blob/main/NER_Live_Coding_NER_to_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cd3a8e3",
      "metadata": {
        "id": "8cd3a8e3"
      },
      "source": [
        "\n",
        "# üß™ Live Coding (90 min): Named Entity Recognition (NER) + Co‚ÄëMention Networks\n",
        "\n",
        "**Goal:** Run spaCy NER on a DataFrame of Factiva news articles (one article per row), turn entities into tidy tables, and build a **person‚Äëto‚Äëperson co‚Äëmention network** (with quick centrality + interactive plot).  \n",
        "We‚Äôll keep the **policy lens** throughout: who are the key actors, which coalitions appear, how do connections shift?\n",
        "\n",
        "**Roadmap (approx.):**\n",
        "- **00‚Äì10 min** ‚Äî Why NER for policy (stakeholder mapping, influence networks, crisis response, sanctions/compliance, media framing).  \n",
        "- **10‚Äì65 min** ‚Äî Live coding: setup ‚Üí NER ‚Üí tidy tables ‚Üí QA ‚Üí co‚Äëmention network ‚Üí centralities ‚Üí Plotly graph ‚Üí export.  \n",
        "- **65‚Äì85 min** ‚Äî Policy applications, pitfalls/bias, validation.  \n",
        "- **85‚Äì90 min** ‚Äî Mini scavenger hunt discussion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† What Is Named Entity Recognition (NER)?\n",
        "\n",
        "| **Aspect**                                      | **Explanation**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
        "| ----------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Definition**                                  | **Named Entity Recognition (NER)** is a sub-task of *Natural Language Processing (NLP)* that automatically identifies and labels key ‚Äúentities‚Äù in text ‚Äî such as **people, organizations, locations, dates, and events**.                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
        "| **Goal**                                        | To turn unstructured text into structured data by extracting *who*, *where*, and *what* from sentences.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
        "| **Origins**                                     | Modern NER grew out of early information-extraction research funded by the **Defense Advanced Research Projects Agency (DARPA)** in the late 1980s‚Äì1990s. DARPA‚Äôs **Message Understanding Conferences (MUC)** created shared tasks and datasets for entity recognition, laying the foundation for today‚Äôs NLP models.                                                                                                                                                                                                                                                                                                                                                  |\n",
        "| **Core Models Today**                           | Most NER systems use transformer or statistical models trained on labeled corpora (e.g., spaCy‚Äôs `en_core_web_md`, BERT-based models, etc.) to tag entity spans with categories like `PERSON`, `ORG`, or `GPE` (geo-political entity).                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
        "| **Why It Matters**                              | NER transforms large text collections (news, policy documents, social media, reports) into analyzable networks of actors, institutions, and issues.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
        "| **Policy & International Affairs Applications** | ‚Ä¢ **Stakeholder mapping:** Identify government officials, NGOs, and corporations in policy debates.<br>‚Ä¢ **Crisis response:** Detect which organizations and regions are repeatedly mentioned in disaster coverage.<br>‚Ä¢ **Disinformation tracking:** Map who or what is being referenced across misinformation networks.<br>‚Ä¢ **Diplomatic analysis:** Trace co-mentions of leaders across international press to understand alliances or tensions.<br>‚Ä¢ **Regulatory research:** Extract firms and sectors from financial or environmental regulations.<br>‚Ä¢ **Legislative studies:** Track which policymakers or agencies appear together across bills or hearings. |\n",
        "| **Limitations**                                 | NER models may confuse similar names, miss multilingual variants, or reflect bias from their training data‚Äîso manual validation and name disambiguation (as you‚Äôll practice) are essential.                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n"
      ],
      "metadata": {
        "id": "1KZJM6bMFbVL"
      },
      "id": "1KZJM6bMFbVL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### üíæ **Did You Know? NER Can Build Databases**\n",
        "> Named Entity Recognition isn‚Äôt just for tagging text ‚Äî it‚Äôs a bridge between *unstructured language* and *structured data*.  \n",
        ">\n",
        "> When you extract entities (like people, organizations, and places) from thousands of documents, you can:\n",
        ">\n",
        "> 1. **Normalize names** ‚Äì merge duplicates or variants (e.g., ‚ÄúU.S.‚Äù ‚Üí ‚ÄúUnited States‚Äù).  \n",
        "> 2. **Store entities** ‚Äì save them as rows in a database or DataFrame, with columns for `entity`, `type`, `source_doc`, `date`, or `context`.  \n",
        "> 3. **Add relationships** ‚Äì connect entities that co-occur in the same article, sentence, or policy section.  \n",
        "> 4. **Query and analyze** ‚Äì use SQL, pandas, or network tools to ask:  \n",
        ">    - Who are the most frequently mentioned actors?  \n",
        ">    - Which organizations are linked to specific policy issues?  \n",
        ">    - How do these connections evolve over time?\n",
        ">\n",
        "> ‚ûú **In practice:** NER lets analysts turn raw text into a relational database ‚Äî a structured map of *who, what, and where* across an entire corpus of policy or media documents.\n"
      ],
      "metadata": {
        "id": "Kav_CVYdFnM_"
      },
      "id": "Kav_CVYdFnM_"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CLEAN & PIN (run this first) ---\n",
        "%pip install -U \"pip<24.3\" setuptools wheel\n",
        "\n",
        "# Remove packages that force or prefer NumPy 2.x (not needed for this class)\n",
        "%pip uninstall -y pytensor opencv-python opencv-contrib-python opencv-python-headless numba cupy-cuda12x tensorflow\n",
        "\n",
        "# Satisfy IPython's dependency\n",
        "%pip install \"jedi>=0.18.0\"\n",
        "\n",
        "# Pin a NumPy that is ABI-compatible with spaCy wheels\n",
        "%pip install \"numpy==1.26.4\"\n",
        "\n",
        "# Now install the libraries we actually need\n",
        "%pip install \"spacy==3.7.4\" \"pandas<2.2\" \"networkx>=3.2\" \"plotly>=5.18\"\n",
        "\n",
        "# Download a medium English model for better NER\n",
        "!python -m spacy download en_core_web_md -q\n",
        "\n",
        "print(\"‚úÖ Clean install complete. NOW go to Runtime ‚Üí Restart runtime, then run the next cell.\")\n"
      ],
      "metadata": {
        "id": "d8f4JomaoSU4"
      },
      "id": "d8f4JomaoSU4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports (post-restart) ---\n",
        "import sys, re, numpy as np, pandas as pd, spacy, networkx as nx, plotly.graph_objects as go\n",
        "from itertools import combinations\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "nlp.max_length = 2_000_000  # or 3_000_000 for extra headroom\n",
        "\n",
        "# Configure pandas display\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "\n",
        "# Verify environment\n",
        "print(\"‚úÖ Environment ready\")\n",
        "print(f\"Python: {sys.version.split()[0]}\")\n",
        "print(f\"NumPy: {np.__version__}\")\n",
        "print(f\"spaCy: {spacy.__version__}\")\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"networkx: {nx.__version__}\")\n"
      ],
      "metadata": {
        "id": "sAM33GZhpJBg"
      },
      "id": "sAM33GZhpJBg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HOMAGYJyo35F"
      },
      "id": "HOMAGYJyo35F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6fc54e1a",
      "metadata": {
        "id": "6fc54e1a"
      },
      "source": [
        "\n",
        "## 1) Load your Factiva articles\n",
        "\n",
        "- Expect a CSV/Parquet with a **`text`** column (one article per row).  \n",
        "- Optional helpful columns: `article_id`, `date`, `source`, `section`, `headline`.  \n",
        "- Replace the demo data below with your real path.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If using Colab, mount Drive (optional but recommended so outputs persist)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "G0CycQ_CvQbQ"
      },
      "id": "G0CycQ_CvQbQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f299353",
      "metadata": {
        "id": "0f299353"
      },
      "outputs": [],
      "source": [
        "\n",
        "# üîÅ Replace this with your actual load (e.g., from Drive)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/factiva_ner_project/factiva.csv\")\n",
        "\n",
        "# Demo placeholder (remove once real data is loaded)\n",
        "# df = pd.DataFrame({\n",
        "#     \"article_id\": [1,2,3],\n",
        "#     \"text\": [\n",
        "#         \"President Biden met with Ursula von der Leyen in Washington to discuss trade and AI.\",\n",
        "#         \"Elon Musk and Tim Cook appeared in Brussels at an EU competition hearing with Margrethe Vestager.\",\n",
        "#         \"The IMF and World Bank met in Marrakech; Kristalina Georgieva spoke with Janet Yellen.\"\n",
        "#     ],\n",
        "#     \"source\": [\"ExampleWire\",\"ExampleWire\",\"ExampleWire\"],\n",
        "#     \"date\": [\"2024-09-10\",\"2024-09-12\",\"2024-10-01\"]\n",
        "# })\n",
        "assert \"CombinedText\" in df.columns and df[\"CombinedText\"].notna().all(), \"Data must include a non-null 'text' column.\"\n",
        "print(df.head(3))\n",
        "print(f\"Articles loaded: {len(df)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56254c15",
      "metadata": {
        "id": "56254c15"
      },
      "source": [
        "\n",
        "## 2) Minimal cleaning + run NER (spaCy)\n",
        "\n",
        "We keep cleaning light for NER (avoid over-normalizing names). We‚Äôll batch with `nlp.pipe(...)` for speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0c1eb32",
      "metadata": {
        "id": "a0c1eb32"
      },
      "outputs": [],
      "source": [
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", str(s)).strip()\n",
        "\n",
        "df[\"text_clean\"] = df[\"CombinedText\"].map(clean_text)\n",
        "\n",
        "def iter_docs(texts, batch_size=32):\n",
        "    for doc in nlp.pipe(texts, batch_size=batch_size, disable=[\"lemmatizer\",\"textcat\"]):\n",
        "        yield doc\n",
        "\n",
        "docs = list(iter_docs(df[\"text_clean\"]))\n",
        "print(\"Docs processed:\", len(docs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7e14a70",
      "metadata": {
        "id": "d7e14a70"
      },
      "source": [
        "\n",
        "## 3) Extract entities ‚Üí tidy tables\n",
        "\n",
        "Keep `PERSON`, `ORG`, `GPE` for policy mapping; attach `article_id` for traceability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e20c3d3",
      "metadata": {
        "id": "5e20c3d3"
      },
      "outputs": [],
      "source": [
        "\n",
        "KEEP = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "\n",
        "rows = []\n",
        "# Use article_id if present; else fallback to row index\n",
        "if \"article_id\" not in df.columns:\n",
        "    df[\"article_id\"] = np.arange(1, len(df)+1)\n",
        "\n",
        "for art_id, doc in zip(df[\"article_id\"], docs):\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in KEEP:\n",
        "            rows.append({\n",
        "                \"article_id\": art_id,\n",
        "                \"entity_raw\": ent.text,\n",
        "                \"label\": ent.label_,\n",
        "                \"start\": ent.start_char,\n",
        "                \"end\": ent.end_char\n",
        "            })\n",
        "ents_df = pd.DataFrame(rows)\n",
        "\n",
        "# Normalize a little (preserve display form too)\n",
        "ents_df[\"entity_norm\"] = (ents_df[\"entity_raw\"]\n",
        "                          .str.strip()\n",
        "                          .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "                          .str.replace(r\"[‚Äô'`]\", \"'\", regex=True))\n",
        "ents_df[\"entity_key\"] = ents_df[\"entity_norm\"].str.lower()\n",
        "\n",
        "# Canonical display casing (most frequent)\n",
        "canonical = (ents_df.groupby(\"entity_key\")[\"entity_norm\"]\n",
        "             .agg(lambda x: x.value_counts().idxmax())\n",
        "             .rename(\"entity\"))\n",
        "ents_df = ents_df.merge(canonical, on=\"entity_key\", how=\"left\")\n",
        "\n",
        "print(\"Entities extracted:\", len(ents_df))\n",
        "ents_df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b66cee4e",
      "metadata": {
        "id": "b66cee4e"
      },
      "source": [
        "\n",
        "## 4) Quick QA / sanity checks\n",
        "\n",
        "Look at frequent entities per type. Expect some noise (titles, partial names, acronyms).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8fe5d23",
      "metadata": {
        "id": "a8fe5d23"
      },
      "outputs": [],
      "source": [
        "\n",
        "def top_vals(df_, label, n=15):\n",
        "    s = (df_[df_[\"label\"]==label][\"entity\"]\n",
        "         .value_counts()\n",
        "         .head(n))\n",
        "    print(f\"\\nTop {label} entities:\")\n",
        "    display(s)\n",
        "\n",
        "for lab in [\"PERSON\",\"ORG\",\"GPE\"]:\n",
        "    top_vals(ents_df, lab, n=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f97d63b",
      "metadata": {
        "id": "3f97d63b"
      },
      "source": [
        "\n",
        "## 5) Build a PERSON‚ÄìPERSON co‚Äëmention network\n",
        "\n",
        "Two people are connected if they appear **in the same article**. (Extension: connect within the same sentence for tighter links.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1527f4",
      "metadata": {
        "id": "9b1527f4"
      },
      "outputs": [],
      "source": [
        "\n",
        "from itertools import combinations\n",
        "\n",
        "persons = ents_df[ents_df[\"label\"]==\"PERSON\"][[\"article_id\",\"entity\"]].drop_duplicates()\n",
        "\n",
        "edge_rows = []\n",
        "for art_id, group in persons.groupby(\"article_id\"):\n",
        "    people = sorted(group[\"entity\"].unique())\n",
        "    for a,b in combinations(people, 2):\n",
        "        edge_rows.append((a,b,art_id))\n",
        "\n",
        "edges_df = pd.DataFrame(edge_rows, columns=[\"src\",\"dst\",\"article_id\"])\n",
        "edge_weights = (edges_df.groupby([\"src\",\"dst\"]).size()\n",
        "                .reset_index(name=\"weight\")\n",
        "                .sort_values(\"weight\", ascending=False))\n",
        "\n",
        "print(edge_weights.head())\n",
        "print(f\"Edges (unique pairs): {len(edge_weights)} | Articles contributing: {edges_df['article_id'].nunique()}\")\n",
        "\n",
        "# Build graph\n",
        "G = nx.Graph()\n",
        "for p in persons[\"entity\"].unique():\n",
        "    G.add_node(p, type=\"PERSON\")\n",
        "for _, row in edge_weights.iterrows():\n",
        "    G.add_edge(row[\"src\"], row[\"dst\"], weight=int(row[\"weight\"]))\n",
        "\n",
        "print(f\"Graph -> Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2229eae1",
      "metadata": {
        "id": "2229eae1"
      },
      "source": [
        "\n",
        "## 6) Centrality: who matters? who bridges?\n",
        "\n",
        "Degree centrality (connectivity), betweenness (bridging), and weighted degree (total co‚Äëmentions).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5181e96a",
      "metadata": {
        "id": "5181e96a"
      },
      "outputs": [],
      "source": [
        "\n",
        "deg = nx.degree_centrality(G)\n",
        "btw = nx.betweenness_centrality(G, normalized=True, weight=\"weight\")\n",
        "deg_w = {n: sum(d[\"weight\"] for _,_,d in G.edges(n, data=True)) for n in G.nodes()}\n",
        "\n",
        "cent_df = (pd.DataFrame({\n",
        "    \"entity\": list(G.nodes()),\n",
        "    \"degree_centrality\": [deg[n] for n in G.nodes()],\n",
        "    \"betweenness\": [btw[n] for n in G.nodes()],\n",
        "    \"weighted_degree\": [deg_w[n] for n in G.nodes()],\n",
        "}).sort_values([\"weighted_degree\",\"degree_centrality\"], ascending=False)\n",
        "  .reset_index(drop=True))\n",
        "\n",
        "cent_df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "# --- 1) Start from your entities table ---\n",
        "# Expect ents_df with columns: [\"article_id\",\"entity\",\"label\", ...]\n",
        "people = ents_df[ents_df[\"label\"]==\"PERSON\"].copy()\n",
        "\n",
        "# --- 2) Normalization helpers ---\n",
        "HONORIFICS = r\"(president|pres\\.|gov\\.|governor|sen\\.|senator|rep\\.|representative|mr\\.|mrs\\.|ms\\.|dr\\.|mayor)\"\n",
        "HONOR_RE = re.compile(rf\"^\\s*{HONORIFICS}\\s+\", re.IGNORECASE)\n",
        "\n",
        "def strip_accents(s: str) -> str:\n",
        "    return \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
        "\n",
        "def normalize_person(name: str) -> str:\n",
        "    if not isinstance(name, str):\n",
        "        return \"\"\n",
        "    n = strip_accents(name).strip()\n",
        "    n = HONOR_RE.sub(\"\", n)                      # drop titles\n",
        "    n = re.sub(r\"[‚Äú‚Äù\\\"(),]\", \"\", n)              # punctuation noise\n",
        "    n = re.sub(r\"\\b[A-Z]\\.\\b\", \"\", n)            # drop middle initials like \"R.\"\n",
        "    n = re.sub(r\"\\s+\", \" \", n).strip()\n",
        "    # Title case but keep common particles intact\n",
        "    n = \" \".join(w.capitalize() if w.lower() not in {\"von\",\"van\",\"de\",\"del\",\"di\",\"la\"} else w.lower()\n",
        "                 for w in n.split())\n",
        "    return n\n",
        "\n",
        "people[\"name_norm\"] = people[\"entity\"].map(normalize_person)\n",
        "\n",
        "# --- 3) Split into full vs last-only ---\n",
        "def last_name(s: str) -> str:\n",
        "    parts = s.split()\n",
        "    return parts[-1] if parts else \"\"\n",
        "\n",
        "def is_last_only(s: str) -> bool:\n",
        "    return len(s.split()) == 1\n",
        "\n",
        "people[\"last\"] = people[\"name_norm\"].map(last_name)\n",
        "people[\"last_only\"] = people[\"name_norm\"].map(is_last_only)\n",
        "\n",
        "# --- 4) Compute dominant full name per last name (with a dominance threshold) ---\n",
        "fulls = people[~people[\"last_only\"]].copy()\n",
        "# count full-name mentions by last name\n",
        "cand = (fulls\n",
        "        .assign(full=lambda d: d[\"name_norm\"])\n",
        "        .groupby([\"last\",\"full\"])\n",
        "        .size()\n",
        "        .reset_index(name=\"n\"))\n",
        "\n",
        "# per-last totals and dominant candidate\n",
        "totals = cand.groupby(\"last\")[\"n\"].sum().rename(\"total\")\n",
        "dom = (cand.sort_values([\"last\",\"n\"], ascending=[True, False])\n",
        "           .groupby(\"last\").head(1)  # top full per last\n",
        "           .merge(totals, on=\"last\"))\n",
        "dom[\"share\"] = dom[\"n\"] / dom[\"total\"]\n",
        "\n",
        "# choose a threshold; 0.6 = ‚Äúdominant enough‚Äù\n",
        "DOMINANCE_THRESHOLD = 0.6\n",
        "dom_map = (dom[dom[\"share\"] >= DOMINANCE_THRESHOLD]\n",
        "           .set_index(\"last\")[\"full\"]\n",
        "           .to_dict())\n",
        "\n",
        "# --- 5) Map last-only mentions to dominant full (when unambiguous) ---\n",
        "def collapse_name(row):\n",
        "    nm = row[\"name_norm\"]\n",
        "    if row[\"last_only\"]:\n",
        "        last = row[\"last\"]\n",
        "        if last in dom_map:\n",
        "            return dom_map[last]   # map \"Trump\" -> \"Donald Trump\"\n",
        "        else:\n",
        "            return nm              # ambiguous last (e.g., \"Bush\") ‚Üí keep as-is\n",
        "    else:\n",
        "        return nm                  # already full name\n",
        "\n",
        "people[\"name_clean\"] = people.apply(collapse_name, axis=1)\n",
        "\n",
        "# (Optional) See what changed\n",
        "# people.loc[people[\"name_norm\"]!=people[\"name_clean\"], [\"name_norm\",\"name_clean\"]].drop_duplicates().head(20)\n"
      ],
      "metadata": {
        "id": "LAFeD8rky7rO"
      },
      "id": "LAFeD8rky7rO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild PERSON table with cleaned names\n",
        "persons_clean = (people[[\"article_id\",\"name_clean\"]]\n",
        "                 .rename(columns={\"name_clean\":\"entity\"})\n",
        "                 .drop_duplicates())\n",
        "\n",
        "# Recompute co-mentions with cleaned entities\n",
        "from itertools import combinations\n",
        "edge_rows = []\n",
        "for art_id, grp in persons_clean.groupby(\"article_id\"):\n",
        "    ppl = sorted(grp[\"entity\"].unique())\n",
        "    for a, b in combinations(ppl, 2):\n",
        "        edge_rows.append((a, b, art_id))\n",
        "\n",
        "edges_df = pd.DataFrame(edge_rows, columns=[\"src\",\"dst\",\"article_id\"])\n",
        "edge_weights = (edges_df.groupby([\"src\",\"dst\"]).size()\n",
        "                .reset_index(name=\"weight\")\n",
        "                .sort_values(\"weight\", ascending=False))\n",
        "\n",
        "# Build graph and recalc centralities\n",
        "G = nx.Graph()\n",
        "for p in persons_clean[\"entity\"].unique():\n",
        "    G.add_node(p, type=\"PERSON\")\n",
        "for _, r in edge_weights.iterrows():\n",
        "    G.add_edge(r[\"src\"], r[\"dst\"], weight=int(r[\"weight\"]))\n",
        "\n",
        "deg = nx.degree_centrality(G)\n",
        "btw = nx.betweenness_centrality(G, normalized=True, weight=\"weight\")\n",
        "deg_w = {n: sum(d[\"weight\"] for _,_,d in G.edges(n, data=True)) for n in G.nodes()}\n",
        "\n",
        "cent_df = (pd.DataFrame({\n",
        "    \"entity\": list(G.nodes()),\n",
        "    \"degree_centrality\": [deg[n] for n in G.nodes()],\n",
        "    \"betweenness\": [btw[n] for n in G.nodes()],\n",
        "    \"weighted_degree\": [deg_w[n] for n in G.nodes()],\n",
        "}).sort_values([\"weighted_degree\",\"degree_centrality\"], ascending=False)\n",
        "  .reset_index(drop=True))\n",
        "\n",
        "cent_df.head(10)\n"
      ],
      "metadata": {
        "id": "H6AY-M_ZzLNw"
      },
      "id": "H6AY-M_ZzLNw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "29f28431",
      "metadata": {
        "id": "29f28431"
      },
      "source": [
        "\n",
        "## 7) Interactive network (Plotly)\n",
        "\n",
        "Small/medium corpora render fine inline. For larger projects, export to **Gephi**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d4300cc",
      "metadata": {
        "id": "8d4300cc"
      },
      "outputs": [],
      "source": [
        "# === Show only the TOP-K strongest relationships (by edge \"weight\") ===\n",
        "TOP_K = 20  # change as needed\n",
        "\n",
        "# 1) Pick top-K edges by weight\n",
        "edges_sorted = sorted(\n",
        "    G.edges(data=True),\n",
        "    key=lambda e: e[2].get(\"weight\", 1),\n",
        "    reverse=True\n",
        ")\n",
        "top_edges = edges_sorted[:TOP_K]\n",
        "\n",
        "# 2) Build a subgraph with just those edges (and their incident nodes)\n",
        "H = nx.Graph()\n",
        "H.add_nodes_from(G.nodes(data=True))  # keep node attrs if any\n",
        "for u, v, d in top_edges:\n",
        "    H.add_edge(u, v, **d)\n",
        "\n",
        "# Optional: remove isolated nodes (if any) that snuck in without edges\n",
        "H.remove_nodes_from(list(nx.isolates(H)))\n",
        "\n",
        "# 3) Recompute layout and centralities on the subgraph\n",
        "pos = nx.spring_layout(H, k=0.6, seed=42, weight=\"weight\")\n",
        "\n",
        "deg_cen_map = nx.degree_centrality(H)\n",
        "btw_map = nx.betweenness_centrality(H, normalized=True, weight=\"weight\")\n",
        "wdeg_map = {n: sum(d[\"weight\"] for _,_,d in H.edges(n, data=True)) for n in H.nodes()}\n",
        "\n",
        "# 4) Build Plotly traces (edges first)\n",
        "edge_x, edge_y = [], []\n",
        "for u, v, d in H.edges(data=True):\n",
        "    x0, y0 = pos[u]; x1, y1 = pos[v]\n",
        "    edge_x += [x0, x1, None]\n",
        "    edge_y += [y0, y1, None]\n",
        "\n",
        "edge_trace = go.Scatter(\n",
        "    x=edge_x, y=edge_y, mode='lines',\n",
        "    line=dict(width=0.5),\n",
        "    hoverinfo='none'\n",
        ")\n",
        "\n",
        "# 5) Nodes\n",
        "node_x = [pos[n][0] for n in H.nodes()]\n",
        "node_y = [pos[n][1] for n in H.nodes()]\n",
        "node_sizes = [8 + 12*deg_cen_map.get(n, 0) for n in H.nodes()]\n",
        "node_text = [\n",
        "    f\"{n}<br>degree={deg_cen_map.get(n,0):.3f}\"\n",
        "    f\"<br>betweenness={btw_map.get(n,0):.3f}\"\n",
        "    f\"<br>w_degree={wdeg_map.get(n,0)}\"\n",
        "    for n in H.nodes()\n",
        "]\n",
        "\n",
        "node_trace = go.Scatter(\n",
        "    x=node_x, y=node_y, mode='markers+text',\n",
        "    text=[n for n in H.nodes()],\n",
        "    textposition=\"top center\",\n",
        "    marker=dict(size=node_sizes),\n",
        "    hovertext=node_text, hoverinfo='text'\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=[edge_trace, node_trace])\n",
        "fig.update_layout(\n",
        "    title=f\"Top {min(TOP_K, H.number_of_edges())} Person‚ÄìPerson Relationships (by co-mentions)\",\n",
        "    showlegend=False, height=640,\n",
        "    margin=dict(l=20, r=20, t=50, b=20),\n",
        "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf47920b",
      "metadata": {
        "id": "cf47920b"
      },
      "source": [
        "\n",
        "## 8) Export tables for reuse (CSV) + Gephi (GEXF)\n",
        "\n",
        "- Nodes: centralities per person  \n",
        "- Edges: aggregated co‚Äëmentions (weights)  \n",
        "- GEXF: open in **Gephi** for deeper analysis (layouts, communities).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18bf12ae",
      "metadata": {
        "id": "18bf12ae"
      },
      "outputs": [],
      "source": [
        "\n",
        "# nodes_out = cent_df.copy()\n",
        "# edges_out = edge_weights.copy()\n",
        "\n",
        "# nodes_out.to_csv(\"ner_network_nodes.csv\", index=False)\n",
        "# edges_out.to_csv(\"ner_network_edges.csv\", index=False)\n",
        "\n",
        "# nx.write_gexf(G, \"ner_network.gexf\")\n",
        "\n",
        "# print(\"Saved: ner_network_nodes.csv, ner_network_edges.csv, ner_network.gexf\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd4a8bb5",
      "metadata": {
        "id": "bd4a8bb5"
      },
      "source": [
        "\n",
        "## 9) Policy applications (talk track)\n",
        "\n",
        "- **Stakeholder mapping:** Who are central actors in AI governance, climate finance, or migration coverage this month?  \n",
        "- **Influence & agenda setting:** Which people bridge communities (high betweenness) between industry and regulators?  \n",
        "- **Sanctions/compliance:** Co‚Äëappearances of sanctioned individuals with firms/financial institutions.  \n",
        "- **Crisis response:** During disasters or epidemics, who coordinates (centrality hotspots) across NGOs/governments?  \n",
        "- **Media framing:** Compare networks by outlet/region to see different coalitions or narratives.\n",
        "  \n",
        "**Caveats & fairness:**  \n",
        "- NER is imperfect (name variants, titles, acronyms). Validate high‚Äëstakes claims.  \n",
        "- Prominence bias: centrality can reflect media attention, not true influence.  \n",
        "- Consider sentence‚Äëlevel ties for stricter edges; add ORG/GPE for multiplex networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecb81c89",
      "metadata": {
        "id": "ecb81c89"
      },
      "source": [
        "\n",
        "## 10) üéØ Mini Scavenger Hunt (discussion, not graded)\n",
        "\n",
        "1) **Central Player:** Who has the highest *weighted degree*? Does that match your expectations?  \n",
        "2) **Bridge Builder:** Top‚Äë3 *betweenness* nodes ‚Äî what communities might they connect?  \n",
        "3) **Outlet Split (optional):** If you have a `source` column, build separate graphs per source ‚Äî what changes?  \n",
        "4) **Time Slice (optional):** Group articles by month/quarter; recompute centralities ‚Äî any event‚Äëdriven shifts?  \n",
        "5) **Tighten the link (stretch):** Redefine edges as co‚Äëmentions **within the same sentence** only; how does the network change?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30246563",
      "metadata": {
        "id": "30246563"
      },
      "source": [
        "\n",
        "## 11) Optional extensions\n",
        "\n",
        "- Add **ORG** and **GPE** nodes for a **tripartite** person‚Äìorg‚Äìplace network.  \n",
        "- Use `en_core_web_trf` (transformer model) for higher‚Äëaccuracy NER (GPU recommended).  \n",
        "- Deduplicate entity variants with light heuristics or entity linking (e.g., Wikipedia/Wikidata).  \n",
        "- Community detection (e.g., Louvain) to find actor clusters.  \n",
        "- Compare networks across **time windows** or **outlets** for framing analysis.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}